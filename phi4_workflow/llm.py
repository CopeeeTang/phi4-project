import requests
import torch
import os
import io
from PIL import Image
import soundfile as sf
from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig
from urllib.request import urlopen
import pandas as pd
import time

class PhiUserIntentWorkflow:
    def __init__(self, model_path="/home/lab/phi4/phi4", verbose=True):
        """
        åˆå§‹åŒ–å¤šæ¨¡æ€LLMæ¨ç†å·¥ä½œæµ
        
        Args:
            model_path: Phi-4æ¨¡å‹è·¯å¾„
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†è°ƒè¯•ä¿¡æ¯
        """
        print("ğŸ”„ åˆå§‹åŒ–æ¨¡å‹...")
        self.processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path, 
            device_map="cuda", 
            torch_dtype="auto", 
            trust_remote_code=True,
            _attn_implementation='flash_attention_2',
        ).cuda()
        self.generation_config = GenerationConfig.from_pretrained(model_path)
        
        # å®šä¹‰æç¤ºè¯ç»“æ„
        self.user_prompt = '<|user|>'
        self.assistant_prompt = '<|assistant|>'
        self.prompt_suffix = '<|end|>'
        self.verbose = verbose
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def log(self, message, level="INFO"):
        """è°ƒè¯•è¾“å‡ºå‡½æ•°"""
        if self.verbose:
            if level == "INFO":
                print(f"â„¹ï¸ {message}")
            elif level == "STEP":
                print(f"\nğŸ”¶ {message}")
            elif level == "SUCCESS":
                print(f"âœ… {message}")
            elif level == "ERROR":
                print(f"âŒ {message}")
            elif level == "TIME":
                print(f"â±ï¸ {message}")
    
    def call_model(self, prompt, image=None, max_new_tokens=500):
        """è°ƒç”¨phi4æ¨¡å‹è¿›è¡Œæ¨ç†"""
        self.log(f"æç¤ºè¯: {prompt[:50]}...", "INFO")
        
        # å¤„ç†è¾“å…¥
        inputs = self.processor(
            text=prompt,
            images=image,
            return_tensors='pt'
        ).to('cuda:0')
        
        # è®¡æ—¶å¹¶ç”Ÿæˆå“åº”
        start_time = time.time()
        generate_ids = self.model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            generation_config=self.generation_config,
        )
        generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]
        response = self.processor.batch_decode(
            generate_ids,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=False
        )[0]
        end_time = time.time()
        
        response_time = end_time - start_time
        self.log(f"å“åº”ç”¨æ—¶: {response_time:.2f}ç§’", "TIME")
        self.log(f"æ¨¡å‹å“åº”: {response}", "INFO")
        
        return response, response_time
    
    def crop_image_at_gaze(self, image, coordinates, radius, save_path=None):
        """æ ¹æ®çœ¼åŠ¨åæ ‡å’ŒåŠå¾„è£å‰ªå›¾åƒ
        
        Args:
            image: PILå›¾åƒå¯¹è±¡
            coordinates: çœ¼åŠ¨åæ ‡å­—å…¸,åŒ…å«xå’Œy
            radius: è£å‰ªåŠå¾„
            save_path: å¯é€‰,ä¿å­˜è£å‰ªå›¾åƒçš„è·¯å¾„
        """
        width, height = image.size
        x_pixel = int(coordinates["x"] * width)
        y_pixel = int(coordinates["y"] * height)
        r_pixel = int(radius * min(width, height))
        
        left = max(0, x_pixel - r_pixel)
        top = max(0, y_pixel - r_pixel)
        right = min(width, x_pixel + r_pixel)
        bottom = min(height, y_pixel + r_pixel)
        
        self.log(f"è£å‰ªåæ ‡: ({left}, {top}, {right}, {bottom})", "INFO")
        cropped_image = image.crop((left, top, right, bottom))
        
        # å¦‚æœæä¾›äº†ä¿å­˜è·¯å¾„,åˆ™ä¿å­˜è£å‰ªåçš„å›¾åƒ
        if save_path:
            try:
                # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
                os.makedirs(os.path.dirname(save_path), exist_ok=True)
                cropped_image.save(save_path)
                self.log(f"è£å‰ªå›¾åƒå·²ä¿å­˜è‡³: {save_path}", "INFO")
            except Exception as e:
                self.log(f"ä¿å­˜è£å‰ªå›¾åƒå¤±è´¥: {str(e)}", "ERROR")
                
        return cropped_image

    def infer_intent(self, image_path, gesture, gaze_data=None):
        """
        å®Œæ•´çš„æ„å›¾æ¨ç†å·¥ä½œæµ
        
        Args:
            image_path: UIå›¾åƒè·¯å¾„
            gesture: ç”¨æˆ·æ‰‹åŠ¿ (å¦‚ 'pinch', 'thumb up')
            gaze_data: çœ¼åŠ¨æ•°æ®åˆ—è¡¨, æ¯ä¸ªåŒ…å«åæ ‡å’ŒåŠå¾„
                æ ¼å¼: [{'coordinates': {'x': 0.5, 'y': 0.5}, 'radius': 0.1}, ...]
        
        Returns:
            åŒ…å«åˆ†æç»“æœçš„å­—å…¸
        """
        self.log(f"å¼€å§‹å¤„ç†å›¾åƒ: {os.path.basename(image_path)}", "STEP")
        
        try:
            # æ­¥éª¤1: åŠ è½½å›¾åƒ
            full_image = Image.open(image_path)
            self.log(f"å›¾åƒå°ºå¯¸: {full_image.size[0]}x{full_image.size[1]}", "INFO")
        except Exception as e:
            self.log(f"åŠ è½½å›¾åƒå¤±è´¥: {str(e)}", "ERROR")
            return {"error": str(e)}
        
        # æ­¥éª¤2: åˆ†ææ•´ä½“UIç•Œé¢
        self.log("åˆ†ææ•´ä½“UIç•Œé¢", "STEP")
        ui_prompt = f'''{self.user_prompt}<|image_1|>
åˆ†æç•Œé¢:
ä¸€å¥è¯æè¿°å½“å‰UIé¡µé¢çš„åŠŸèƒ½å’Œä¸»è¦ç»„ä»¶ã€‚
{self.prompt_suffix}{self.assistant_prompt}'''
        
        overview, overview_time = self.call_model(ui_prompt, full_image, 100)
        self.log(f"UIæ¦‚è§ˆ: {overview}", "SUCCESS")
        
        # æ­¥éª¤3: è£å‰ªçœ¼åŠ¨å…³æ³¨åŒºåŸŸçš„å›¾åƒ
        cropped_images = []
        if gaze_data and len(gaze_data) > 0:
            self.log(f"è£å‰ª{len(gaze_data)}ä¸ªçœ¼åŠ¨å…³æ³¨åŒºåŸŸ", "STEP")
            
            for i, gaze in enumerate(gaze_data):
                self.log(f"è£å‰ªçœ¼åŠ¨ç‚¹ #{i+1}: åæ ‡({gaze['coordinates']['x']:.2f}, {gaze['coordinates']['y']:.2f})", "INFO")
                
                # è£å‰ªçœ¼åŠ¨æ³¨è§†åŒºåŸŸ
                cropped_image = self.crop_image_at_gaze(
                    full_image, 
                    gaze['coordinates'], 
                    gaze['radius']
                )
                
                cropped_images.append({
                    'gaze_id': i+1,
                    'coordinates': gaze['coordinates'],
                    'cropped_image': cropped_image
                })
                
                self.log(f"çœ¼åŠ¨åŒºåŸŸ #{i+1} è£å‰ªå®Œæˆ", "SUCCESS")
        else:
            self.log("æœªæä¾›çœ¼åŠ¨æ•°æ®ï¼Œè·³è¿‡å›¾åƒè£å‰ªæ­¥éª¤", "INFO")
        
        # æ­¥éª¤4: ç»“åˆUIå’Œæ‰‹åŠ¿æ¨æ–­æ„å›¾ (ä½¿ç”¨ç®€åŒ–çš„æç¤ºè¯)
        self.log(f"ä½¿ç”¨ç®€åŒ–æç¤ºè¯æ¨æ–­æ„å›¾", "STEP")
        
        intent_prompt = f'''
        <|user|><|image_1|>
        UIç•Œé¢: {overview}
        ç”¨æˆ·æ‰‹åŠ¿: {gesture}
        æ ¹æ®å½“å‰UIé¡µé¢å’Œç”¨æˆ·æ‰‹åŠ¿ï¼Œæ¨æµ‹ç”¨æˆ·å¯èƒ½æƒ³è¦æ‰§è¡Œçš„æ“ä½œã€‚
        <|end|><|assistant|>
        '''
        
        intent, intent_time = self.call_model(intent_prompt, full_image, 150)
        self.log(f"æ¨æ–­æ„å›¾: {intent}", "SUCCESS")
        
        # è¿”å›å®Œæ•´åˆ†æç»“æœ
        result = {
            'image_path': image_path,
            'ui_overview': overview,
            'gesture': gesture,
            'cropped_images': cropped_images,  # åªåŒ…å«è£å‰ªåçš„å›¾åƒï¼Œä¸åšåˆ†æ
            'inferred_intent': intent,
            'total_response_time': overview_time + intent_time
        }
        
        self.log(f"å®Œæˆåˆ†æ! æ€»ç”¨æ—¶: {result['total_response_time']:.2f}ç§’", "STEP")
        return result


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆå§‹åŒ–å·¥ä½œæµ
    workflow = PhiUserIntentWorkflow()
    
    # æµ‹è¯•å•å¼ å›¾åƒ
    print("\n========== å•å›¾åƒæ‰‹åŠ¿å¤„ç†æµ‹è¯• ==========")
    # å®šä¹‰æµ‹è¯•æ•°æ®
    image_path = 'dataset/dataset/rico/16.jpg'
    gesture = 'thumb up'
    
    # çœ¼åŠ¨æ•°æ®ç¤ºä¾‹ (è§„èŒƒåŒ–åæ ‡)
    gaze_data = [
        {
            'coordinates': {'x': 0.5, 'y': 0.3},  # å±å¹•ä¸­ä¸Šéƒ¨
            'radius': 0.15                        # å…³æ³¨åŠå¾„
        },
        {
            'coordinates': {'x': 0.2, 'y': 0.5},  # å±å¹•å·¦ä¾§
            'radius': 0.15
        }
    ]
    
    # è¿è¡Œæ„å›¾åˆ†æ
    result = workflow.infer_intent(image_path, gesture, gaze_data)
    
    # è¾“å‡ºå®Œæ•´ç»“æœ
    print("\n========== æ„å›¾åˆ†æç»“æœ ==========")
    print(f"UIæ¦‚è§ˆ: {result['ui_overview']}")
    print(f"æ‰‹åŠ¿: {result['gesture']}")
    print(f"è£å‰ªåŒºåŸŸæ•°é‡: {len(result['cropped_images'])}")
    print(f"æ¨æ–­æ„å›¾: {result['inferred_intent']}")
    print(f"æ€»å“åº”æ—¶é—´: {result['total_response_time']:.2f}ç§’")